{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bfc848-548d-4309-b770-7e54efedca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d51fd9-2692-47c8-9a6a-a01eeb322b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir, self.labels.iloc[idx, 1])\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        label = self.labels.iloc[idx, 2]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226140f-fbcb-4e8b-9097-096d81adfcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import random_split\n",
    "random.seed(42)\n",
    "\n",
    "dataset = CelebDataset(csv_file='./train.csv', root_dir='./train', transform=transform)\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6459891-c2e2-47dd-bb70-1c12b9f33e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 56 * 56)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "num_classes = 100\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "category = pd.read_csv(\"./category.csv\")  \n",
    "label_encoder.fit(category['Category'])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = CNN(num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.1)\n",
    "state = {\"model\":model.state_dict(), \"op\":optimizer.state_dict()}\n",
    "torch.save(state, \"./model_regularized.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45d464-8849-45c0-aafe-bed4f3bed682",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(5,num_epochs+5):\n",
    "    state = torch.load(f\"./model_regularized.pickle\")\n",
    "    model = CNN(num_classes).to(device)\n",
    "    model.load_state_dict(state['model'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1.25)\n",
    "    optimizer.load_state_dict(state[\"op\"])\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        # inputs =  inputs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        encoded_labels = torch.tensor(label_encoder.transform(labels), dtype=torch.long, device=device)\n",
    "        loss = criterion(outputs, encoded_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += encoded_labels.size(0)\n",
    "        correct_train += (predicted == encoded_labels).sum().item()\n",
    "    \n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "    train_accuracy = correct_train / total_train\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            encoded_labels = torch.tensor(label_encoder.transform(labels), dtype=torch.long, device=device)\n",
    "            loss = criterion(outputs, encoded_labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += encoded_labels.size(0)\n",
    "            correct_val += (predicted == encoded_labels).sum().item()\n",
    "    \n",
    "    val_loss = val_loss / len(val_dataset)\n",
    "    val_accuracy = correct_val / total_val\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {val_loss:.4f}')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_accuracy:.4f}')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {val_accuracy:.4f}')\n",
    "    state = {\"model\":model.state_dict(), \"op\":optimizer.state_dict()}\n",
    "    torch.save(state, f\"./model_regularized.pickle\")\n",
    "\n",
    "\n",
    "\n",
    "print('Training finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36315e-c79f-42dd-81ff-d28bee8757f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(f\"./model_regularized5.pickle\")\n",
    "model = CNN(num_classes).to(device)\n",
    "model.load_state_dict(state['model'])\n",
    "class CelebTestDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.image_filenames = os.listdir(root_dir)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir, self.image_filenames[idx])\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        # Convert image to RGB if it's not already\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.image_filenames[idx]\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "# Create dataset and dataloader for test images\n",
    "test_dataset = CelebTestDataset(root_dir='./test', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(\"Yep\")\n",
    "predicted_labels = []\n",
    "image_names = []\n",
    "model.eval()  \n",
    "with torch.no_grad():\n",
    "    for images, filenames in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_labels.append(predicted.item())\n",
    "        image_names.append(filenames[0])\n",
    "\n",
    "\n",
    "def decode_labels(predicted_labels, label_encoder):\n",
    "    return label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "decoded_labels = decode_labels(predicted_labels, label_encoder)\n",
    "\n",
    "results_df = pd.DataFrame({'Id': image_names, 'Category': decoded_labels})\n",
    "\n",
    "results_df.to_csv('./predicted_test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39094bf-e8e6-4ef4-8ecc-add0762512b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./predicted_test_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb083cc-daf3-4114-b8bd-a39005f2a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Id\"] = df.Id.apply(lambda x: int(x.split(\".\")[0]))\n",
    "\n",
    "df.sort_values(\"Id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e144734f-dea3-4596-9d9c-216cd3f16b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./predicted_test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c2d133-5bf2-4531-af47-d6ee9b2d6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb3fd0-9b78-4e07-9480-1aaebc3e5edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
